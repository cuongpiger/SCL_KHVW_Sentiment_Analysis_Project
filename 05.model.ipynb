{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import modules.eda as Detective\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import emojis\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def expandEmojisDecode(pcomment: str):\n",
    "    expand_emojis = ''\n",
    "    for e in emojis.get(pcomment):\n",
    "        amount = pcomment.count(e)\n",
    "        expand_emojis += (f\"{emojis.decode(e)[1:-1]} \")*amount\n",
    "        \n",
    "    return expand_emojis.strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "reviews = pd.read_csv(\"./data/normalize_reviews.csv\").fillna(\"\")\n",
    "reviews = reviews[['raw_comment', 'normalize_comment', 'emoji', 'label']]\n",
    "\n",
    "reviews.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>normalize_comment</th>\n",
       "      <th>emoji</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Giao haÌ€ng kh Ä‘uÌng caÌ‚Ì€n pheÌ‚ biÌ€nh hjjjjjhhd...</td>\n",
       "      <td>giao haÌ€ng khoÌ‚ng Ä‘uÌng caÌ‚Ì€n pheÌ‚ biÌ€nh</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ChaÌ‚Ìt luÌ›oÌ›Ì£ng saÌ‰n phaÌ‚Ì‰m taÌ£m Ä‘uÌ›oÌ›Ì£c. Giao...</td>\n",
       "      <td>chaÌ‚Ìt luÌ›oÌ›Ì£ng saÌ‰n phaÌ‚Ì‰m taÌ£m Ä‘uÌ›oÌ›Ì£c giao ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ko coÌ laÌ†Ìc tay nhuÌ› hiÌ€nh</td>\n",
       "      <td>khoÌ‚ng coÌ laÌ†Ìc tay nhuÌ› hiÌ€nh</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Giao haÌ€ng laÌ‚u. BaÌ‰o coÌ laÌ†Ìc tay maÌ€ k thaÌ‚...</td>\n",
       "      <td>giao haÌ€ng laÌ‚u baÌ‰o coÌ laÌ†Ìc tay maÌ€ khoÌ‚ng ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MiÌ€nh mua 2 caÌi, moÌ£Ì‚t duÌ€ng ok. MoÌ£Ì‚t caÌi k...</td>\n",
       "      <td>mua caÌi moÌ£Ì‚t duÌ€ng ok moÌ£Ì‚t caÌi khoÌ‚ng chaÌ£...</td>\n",
       "      <td>ğŸ˜¢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         raw_comment  \\\n",
       "0  Giao haÌ€ng kh Ä‘uÌng caÌ‚Ì€n pheÌ‚ biÌ€nh hjjjjjhhd...   \n",
       "1  ChaÌ‚Ìt luÌ›oÌ›Ì£ng saÌ‰n phaÌ‚Ì‰m taÌ£m Ä‘uÌ›oÌ›Ì£c. Giao...   \n",
       "2                        Ko coÌ laÌ†Ìc tay nhuÌ› hiÌ€nh   \n",
       "3  Giao haÌ€ng laÌ‚u. BaÌ‰o coÌ laÌ†Ìc tay maÌ€ k thaÌ‚...   \n",
       "4  MiÌ€nh mua 2 caÌi, moÌ£Ì‚t duÌ€ng ok. MoÌ£Ì‚t caÌi k...   \n",
       "\n",
       "                                   normalize_comment emoji  label  \n",
       "0           giao haÌ€ng khoÌ‚ng Ä‘uÌng caÌ‚Ì€n pheÌ‚ biÌ€nh            0  \n",
       "1  chaÌ‚Ìt luÌ›oÌ›Ì£ng saÌ‰n phaÌ‚Ì‰m taÌ£m Ä‘uÌ›oÌ›Ì£c giao ...            0  \n",
       "2                    khoÌ‚ng coÌ laÌ†Ìc tay nhuÌ› hiÌ€nh            0  \n",
       "3  giao haÌ€ng laÌ‚u baÌ‰o coÌ laÌ†Ìc tay maÌ€ khoÌ‚ng ...            0  \n",
       "4  mua caÌi moÌ£Ì‚t duÌ€ng ok moÌ£Ì‚t caÌi khoÌ‚ng chaÌ£...     ğŸ˜¢      0  "
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hiá»‡n táº¡i, dataset chÃºng ta cÃ³ hai feature lÃ  `normalize_comment` vÃ  `emoji`, chÃºng ta cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t sentiment analysis classifier báº±ng hai cÃ¡ch:\n",
    "  * **CÃ¡ch 1**: ChÃºng ta cÃ³ thá»ƒ Ä‘em feature `emoji` Ä‘á»ƒ thÃªm vÃ o pháº§n Ä‘áº§u hoáº·c cuá»‘i cá»§a feature `normalize_comment`. Sau Ä‘Ã³ tiáº¿n hÃ nh Ä‘Ã o táº¡o má»™t model duy nháº¥t vá»›i input lÃ  feature `normalize_comment` Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o feature `emoji`.\n",
    "    * Æ¯u Ä‘iá»ƒm: QuÃ¡ trÃ¬nh Ä‘Ã o táº¡o model Ä‘á»¡ tá»‘n thá»i gian hÆ¡n do ta chá»‰ thá»±c hiá»‡n build má»™t model duy nháº¥t.\n",
    "    * NhÆ°á»£c Ä‘iá»ƒm:\n",
    "      * CÃ³ kháº£ nÄƒng lÃ m suy giáº£m sá»©c máº¡nh cá»§a model, nhiá»u ngÆ°á»i há» cÃ²n xÃ¢y dÆ°ng riÃªng má»™t **emoji classifier** bá»Ÿi vÃ¬ theo há» nhiá»u khi emoji Ä‘Ã³ mang láº¡i Ã½ nghÄ©a tháº­m chÃ­ cÃ²n cao hÆ¡n so vá»›i ngá»¯ nghÄ©a cá»§a comment Ä‘Ã³. Trong khi viá»‡c Ä‘Ã o táº¡o má»™t emoji classifier Ä‘á»¡ váº¥t vÃ£ hÆ¡n.\n",
    "      * Ta pháº£i tá»‘n thá»i gian vÃ o viá»‡c xÃ¢y dá»±ng thÃªm má»™t model riÃªng sau Ä‘Ã³ láº¡i tá»•ng há»£p káº¿t quáº£ cá»§a emoji classifier model nÃ y vá»›i sentiment classifier model ban Ä‘áº§u.\n",
    "  * **CÃ¡ch 2**: Ta cÃ³ thá»ƒ xÃ¢y dá»±ng hai model riÃªng biá»‡t, má»™t sentiment analysis model Ä‘áº£m nháº­n nhiá»‡m vá»¥ phÃ¢n lá»›p cho cÃ¢u chá»¯ vÃ  má»™t emoji analysis model Ä‘áº£m nháº­n nhiá»‡m vá»¥ phÃ¢n lá»›p trÃªn emoji. Sau Ä‘Ã³ ta káº¿t há»£p hai model nÃ y Ä‘á»ƒ cho ra káº¿t quáº£ phÃ¢n lá»›p cuá»‘i cÃ¹ng.\n",
    "    * Æ¯u Ä‘iá»ƒm: Káº¿t quáº£ phÃ¢n lá»›p cho ra cÃ³ kháº£ nÄƒng chÃ­nh xÃ¡c hÆ¡n, ta cÃ³ thá»ƒ linh hoáº¡t trong viá»‡c thiáº¿p láº­p trá»ng sá»‘ Ä‘á»ƒ Æ°u tiÃªn giÃ¡ trá»‹ cá»§a model nÃ o mang láº¡i lá»£i Ã­ch cao hÆ¡n cho káº¿t quáº£ á»Ÿ bÆ°á»›c káº¿t há»£p hai model láº¡i Ä‘á»ƒ dá»± Ä‘oÃ¡n.\n",
    "    * NhÆ°á»£c Ä‘iá»ƒm:\n",
    "      * Váº¥t váº£ hÆ¡n vá» máº·t tiá»n xá»­ lÃ­ dá»¯ liá»‡u, tá»‘n thÃªm thá»i gian phÃ¢n tÃ­ch vÃ  thiáº¿t káº¿ thÃªm má»™t emoji classifier.\n",
    "      * Pháº§n lá»›n trong dataset hiá»‡n táº¡i, sá»‘ comment chá»©a emoji khÃ´ng nhiá»u. NhÆ° project 2 ta Ä‘Ã£ tháº¥y toÃ n bá»™ dataset cá»§a ta chá»‰ khoáº£ng 1300 comment lÃ  cÃ³ chá»©a emoji kÃ¨m theo, ta cÃ³ thá»ƒ kháº¯c phá»¥c báº±ng má»™t trong nhá»¯ng cÃ¡ch:\n",
    "        * Crawl thÃªm data, nhÆ°ng viá»‡c nÃ y khÃ´ng kháº£ khi. Ta Ä‘Ã£ Ä‘á» cáº­p ráº±ng sá»‘ comment chá»©a emoji ráº¥t Ã­t trong má»™t dataset nÃªn viá»‡c ta hi sinh thá»i gian chá»‰ Ä‘á»ƒ trÃ­ch xuáº¥t má»™t pháº§n nhá» trong má»™t dataset cá»±c lá»›n sau khi crawl vá» cÃ³ thá»ƒ phÃ­ pháº¡m.\n",
    "        * Sá»­ dá»¥ng kÄ© thuáº­t **Data Upsampler** vá»›i package cá»§a IBM `from imblearn.over_sampling import RandomOverSampler`.\n",
    "\n",
    "    $\\Rightarrow$ HÆ°á»›ng giáº£i quyáº¿t: Quáº£ tháº­t ta cÃ³ khoáº£ng 1300 observer thÃ¬ lÃ  má»™t con sá»‘ khÃ´ng quÃ¡ Ã­t cÅ©ng khÃ´ng quÃ¡ nhiá»u, sá»‘ lÆ°á»£ng emoji chá»©a trong má»™t comment cÅ©ng khÃ´ng nhiá»u (khÃ´ng tÃ­nh cÃ¡c duplicate emoji). BÃ i toÃ¡n cá»§a chÃºng ta Ä‘Æ¡n thuáº§n chá»‰ lÃ  phÃ¢n tÃ­ch cÃ¡c emoji ra hai class lÃ  negative vÃ  positive. HÃ£y cÃ¹ng nhÃ¬n láº¡i má»™t dataset ná»•i tiáº¿ng khÃ¡c lÃ  **Iris**, ta cÃ³ 3 class á»Ÿ target variable vÃ  150 observe. Váº­y ta váº«n cÃ³ thá»ƒ build má»™t classifier model Ä‘á»§ tá»‘t náº¿u input cá»§a ta cÅ©ng **Ä‘á»§ tá»‘t**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Emoji sentiment analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "NhÆ° váº­y chiáº¿n lÆ°á»£c Ä‘Ã o táº¡o má»™t Emoji sentiment analysis cá»§a chÃºng ta sáº½ nhÆ° sau:\n",
    "  * BÆ°á»›c 1: Tiáº¿n hÃ nh láº¥y toÃ n bá»™ cÃ¡c emoji trong cÃ¡c comment vÃ  lÆ°u trong cáº¥u trÃºc dá»¯ liá»‡u `set`, má»¥c tiÃªu ta cáº§n biáº¿t cÃ³ bao nhiÃªu unique emoji trong dataset cá»§a chÃºng ta.\n",
    "  * BÆ°á»›c 2: Táº¡o 2 dictionary, má»™t dictionary cÃ³ key lÃ  emoji vÃ  value lÃ  index cá»§a emoji Ä‘Ã³ trong `set`, dictionary thá»© hai cÃ³ key lÃ  index cá»§a emoji trong `set` vÃ  value lÃ  emoji.\n",
    "  * BÆ°á»›c 3: Giáº£ sá»­ cÃ³ $n$ unique emoji trong dataset, ta sáº½ chá»‰ quan tÃ¢m Ä‘áº¿n cÃ¡c comment chá»©a emoji, vÃ  vá»›i tá»«ng comment nhÆ° váº­y ta táº¡o má»™t vector $n$ pháº§n tá»­ sau Ä‘Ã³ Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p **Bag of words** lÃªn vector nÃ y.\n",
    "  * BÆ°á»›c 4: Xem xÃ©t $n$ cÃ³ lá»›n khÃ´ng, náº¿u quÃ¡ lá»›n thÃ¬ cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£m chiá»u dá»¯ liá»‡u.\n",
    "  * BÆ°á»›c 5: DÃ¹ng cÃ¡c vector $n$ pháº§n tá»­ + label cá»§a chÃºng sau khi Ä‘Ã£ qua bÆ°á»›c 4 Ä‘á»ƒ Ä‘Ã o táº¡o má»™t classifier model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "BÃ¢y giá» ta sáº½ tiáº¿n hÃ nh lá»c ra nhá»±ng comment mÃ  cÃ³ chá»©a emoji dá»±a vÃ o feature `emoji`. Sau Ä‘Ã³ lÆ°u cÃ¡c observe nÃ y vÃ o biáº¿n `emojis`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data = reviews[reviews['emoji'] != \"\"]\n",
    "\n",
    "data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>normalize_comment</th>\n",
       "      <th>emoji</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MiÌ€nh mua 2 caÌi, moÌ£Ì‚t duÌ€ng ok. MoÌ£Ì‚t caÌi k...</td>\n",
       "      <td>mua caÌi moÌ£Ì‚t duÌ€ng ok moÌ£Ì‚t caÌi khoÌ‚ng chaÌ£...</td>\n",
       "      <td>ğŸ˜¢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Giao sai maÌ€u sai size coÌ 1 deÌp loÌ‚ng size 3...</td>\n",
       "      <td>giao sai maÌ€u sai size coÌ deÌp loÌ‚ng size aÌ€ ...</td>\n",
       "      <td>ğŸ¤¬</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ÄoÌ›n haÌ€ng Ä‘aÌƒ thanh toaÌn airpay roÌ‚Ì€i maÌ€ sh...</td>\n",
       "      <td>Ä‘oÌ›n haÌ€ng thanh toaÌn maÌ€ giao cho nguÌ›oÌ›Ì€i l...</td>\n",
       "      <td>ğŸ™„</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AÌo croptop freesize roÌ£Ì‚ng voÌ›Ìi miÌ€nh \\nMiÌ€n...</td>\n",
       "      <td>aÌo roÌ£Ì‚ng miÌ€nh kg</td>\n",
       "      <td>ğŸ‘</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>KhaÌ buoÌ‚Ì€n . ÄaÌ£Ì†t 2 caÌi Ä‘eÌ‚Ì€u khoÌ‚ng chaÌ£y ...</td>\n",
       "      <td>khaÌ buoÌ‚Ì€n Ä‘aÌ£Ì†t caÌi Ä‘eÌ‚Ì€u khoÌ‚ng chaÌ£y gioÌ›...</td>\n",
       "      <td>â™€ï¸ ğŸ¤·</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>KhoÌ‚ng mua hoÌ‚Ìi haÌ£Ì‚n Ä‘uÌ›Ì€ng keÌ‚u nhaÌ haha, ...</td>\n",
       "      <td>khoÌ‚ng mua hoÌ‚Ìi haÌ£Ì‚n Ä‘uÌ›Ì€ng keÌ‚u nhaÌ haÌ€ng ...</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>khuyeÌ‚n xinh laÌ†Ìm luÌc miÌ€nh Ä‘aÌ£Ì†t coÌ€n Ä‘uÌ›oÌ›...</td>\n",
       "      <td>khuyeÌ‚n xinh laÌ†Ìm luÌc Ä‘aÌ£Ì†t coÌ€n Ä‘uÌ›oÌ›Ì£c dea...</td>\n",
       "      <td>ğŸ˜˜</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14193</th>\n",
       "      <td>Ä‘eÌ£p reÌ‰ chaÌ‚Ìt luÌ›oÌ›Ì£ng voÌ‚ cuÌ€ng xiÌ£n ğŸ˜—ğŸ˜—ğŸ˜— ma...</td>\n",
       "      <td>Ä‘eÌ£p reÌ‰ chaÌ‚Ìt luÌ›oÌ›Ì£ng voÌ‚ cuÌ€ng xiÌ£n maÌƒi u...</td>\n",
       "      <td>ğŸ˜—</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14194</th>\n",
       "      <td>ÄeÌ£p, seÌƒ uÌ‰ng hoÌ£Ì‚ tieÌ‚Ìp nheÌ ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•...</td>\n",
       "      <td>Ä‘eÌ£p seÌƒ uÌ‰ng hoÌ£Ì‚ tieÌ‚Ìp nheÌ</td>\n",
       "      <td>ğŸ‘•</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14200</th>\n",
       "      <td>Mua veÌ‚Ì€ cho choÌ‚Ì€ng maÌ€ Ä‘eÌ‚Ì‰ con em noÌ maÌ£Ì†c...</td>\n",
       "      <td>mua veÌ‚Ì€ cho choÌ‚Ì€ng maÌ€ con em maÌ£Ì†c thuÌ›Ì‰ tr...</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1283 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_comment  \\\n",
       "4      MiÌ€nh mua 2 caÌi, moÌ£Ì‚t duÌ€ng ok. MoÌ£Ì‚t caÌi k...   \n",
       "15     Giao sai maÌ€u sai size coÌ 1 deÌp loÌ‚ng size 3...   \n",
       "30     ÄoÌ›n haÌ€ng Ä‘aÌƒ thanh toaÌn airpay roÌ‚Ì€i maÌ€ sh...   \n",
       "43     AÌo croptop freesize roÌ£Ì‚ng voÌ›Ìi miÌ€nh \\nMiÌ€n...   \n",
       "47     KhaÌ buoÌ‚Ì€n . ÄaÌ£Ì†t 2 caÌi Ä‘eÌ‚Ì€u khoÌ‚ng chaÌ£y ...   \n",
       "...                                                  ...   \n",
       "14184  KhoÌ‚ng mua hoÌ‚Ìi haÌ£Ì‚n Ä‘uÌ›Ì€ng keÌ‚u nhaÌ haha, ...   \n",
       "14186  khuyeÌ‚n xinh laÌ†Ìm luÌc miÌ€nh Ä‘aÌ£Ì†t coÌ€n Ä‘uÌ›oÌ›...   \n",
       "14193  Ä‘eÌ£p reÌ‰ chaÌ‚Ìt luÌ›oÌ›Ì£ng voÌ‚ cuÌ€ng xiÌ£n ğŸ˜—ğŸ˜—ğŸ˜— ma...   \n",
       "14194  ÄeÌ£p, seÌƒ uÌ‰ng hoÌ£Ì‚ tieÌ‚Ìp nheÌ ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•...   \n",
       "14200  Mua veÌ‚Ì€ cho choÌ‚Ì€ng maÌ€ Ä‘eÌ‚Ì‰ con em noÌ maÌ£Ì†c...   \n",
       "\n",
       "                                       normalize_comment emoji  label  \n",
       "4      mua caÌi moÌ£Ì‚t duÌ€ng ok moÌ£Ì‚t caÌi khoÌ‚ng chaÌ£...     ğŸ˜¢      0  \n",
       "15     giao sai maÌ€u sai size coÌ deÌp loÌ‚ng size aÌ€ ...     ğŸ¤¬      0  \n",
       "30     Ä‘oÌ›n haÌ€ng thanh toaÌn maÌ€ giao cho nguÌ›oÌ›Ì€i l...     ğŸ™„      0  \n",
       "43                                   aÌo roÌ£Ì‚ng miÌ€nh kg     ğŸ‘      0  \n",
       "47     khaÌ buoÌ‚Ì€n Ä‘aÌ£Ì†t caÌi Ä‘eÌ‚Ì€u khoÌ‚ng chaÌ£y gioÌ›...  â™€ï¸ ğŸ¤·      0  \n",
       "...                                                  ...   ...    ...  \n",
       "14184  khoÌ‚ng mua hoÌ‚Ìi haÌ£Ì‚n Ä‘uÌ›Ì€ng keÌ‚u nhaÌ haÌ€ng ...     ğŸ˜‚      1  \n",
       "14186  khuyeÌ‚n xinh laÌ†Ìm luÌc Ä‘aÌ£Ì†t coÌ€n Ä‘uÌ›oÌ›Ì£c dea...     ğŸ˜˜      1  \n",
       "14193  Ä‘eÌ£p reÌ‰ chaÌ‚Ìt luÌ›oÌ›Ì£ng voÌ‚ cuÌ€ng xiÌ£n maÌƒi u...     ğŸ˜—      1  \n",
       "14194                     Ä‘eÌ£p seÌƒ uÌ‰ng hoÌ£Ì‚ tieÌ‚Ìp nheÌ     ğŸ‘•      1  \n",
       "14200  mua veÌ‚Ì€ cho choÌ‚Ì€ng maÌ€ con em maÌ£Ì†c thuÌ›Ì‰ tr...     ğŸ˜‚      1  \n",
       "\n",
       "[1283 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Package **emojis** chá»‰ há»— trá»£ chá»©c nÄƒng tÃ¡ch emoji ra khá»i text, nÃ³ khÃ´ng há»— trá»£ thá»‘ng kÃª lÃ  má»™t emoji xuáº¥t hiá»‡n bao nhiÃªu láº§n trong text Ä‘Ã³. BÃ¢y giá» ta sáº½ tÃ¡ch cÃ¡c emoji trong comment ra nhÆ°ng váº«n báº£o toÃ n vá» máº·t sá»‘ lÆ°á»£ng ban Ä‘áº§u cá»§a nÃ³ trong feature `raw_comment` vÃ  lÆ°u vÃ o feature `emoji`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data['decode_emoji'] = data['raw_comment'].apply(lambda cmt: expandEmojisDecode(cmt))\n",
    "\n",
    "data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>normalize_comment</th>\n",
       "      <th>emoji</th>\n",
       "      <th>label</th>\n",
       "      <th>decode_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MiÌ€nh mua 2 caÌi, moÌ£Ì‚t duÌ€ng ok. MoÌ£Ì‚t caÌi k...</td>\n",
       "      <td>mua caÌi moÌ£Ì‚t duÌ€ng ok moÌ£Ì‚t caÌi khoÌ‚ng chaÌ£...</td>\n",
       "      <td>ğŸ˜¢</td>\n",
       "      <td>0</td>\n",
       "      <td>cry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Giao sai maÌ€u sai size coÌ 1 deÌp loÌ‚ng size 3...</td>\n",
       "      <td>giao sai maÌ€u sai size coÌ deÌp loÌ‚ng size aÌ€ ...</td>\n",
       "      <td>ğŸ¤¬</td>\n",
       "      <td>0</td>\n",
       "      <td>cursing_face cursing_face cursing_face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ÄoÌ›n haÌ€ng Ä‘aÌƒ thanh toaÌn airpay roÌ‚Ì€i maÌ€ sh...</td>\n",
       "      <td>Ä‘oÌ›n haÌ€ng thanh toaÌn maÌ€ giao cho nguÌ›oÌ›Ì€i l...</td>\n",
       "      <td>ğŸ™„</td>\n",
       "      <td>0</td>\n",
       "      <td>roll_eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AÌo croptop freesize roÌ£Ì‚ng voÌ›Ìi miÌ€nh \\nMiÌ€n...</td>\n",
       "      <td>aÌo roÌ£Ì‚ng miÌ€nh kg</td>\n",
       "      <td>ğŸ‘</td>\n",
       "      <td>0</td>\n",
       "      <td>thumbsup thumbsup thumbsup thumbsup thumbsup t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>KhaÌ buoÌ‚Ì€n . ÄaÌ£Ì†t 2 caÌi Ä‘eÌ‚Ì€u khoÌ‚ng chaÌ£y ...</td>\n",
       "      <td>khaÌ buoÌ‚Ì€n Ä‘aÌ£Ì†t caÌi Ä‘eÌ‚Ì€u khoÌ‚ng chaÌ£y gioÌ›...</td>\n",
       "      <td>â™€ï¸ ğŸ¤·</td>\n",
       "      <td>0</td>\n",
       "      <td>shrug female_sign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>KhoÌ‚ng mua hoÌ‚Ìi haÌ£Ì‚n Ä‘uÌ›Ì€ng keÌ‚u nhaÌ haha, ...</td>\n",
       "      <td>khoÌ‚ng mua hoÌ‚Ìi haÌ£Ì‚n Ä‘uÌ›Ì€ng keÌ‚u nhaÌ haÌ€ng ...</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>khuyeÌ‚n xinh laÌ†Ìm luÌc miÌ€nh Ä‘aÌ£Ì†t coÌ€n Ä‘uÌ›oÌ›...</td>\n",
       "      <td>khuyeÌ‚n xinh laÌ†Ìm luÌc Ä‘aÌ£Ì†t coÌ€n Ä‘uÌ›oÌ›Ì£c dea...</td>\n",
       "      <td>ğŸ˜˜</td>\n",
       "      <td>1</td>\n",
       "      <td>kissing_heart kissing_heart kissing_heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14193</th>\n",
       "      <td>Ä‘eÌ£p reÌ‰ chaÌ‚Ìt luÌ›oÌ›Ì£ng voÌ‚ cuÌ€ng xiÌ£n ğŸ˜—ğŸ˜—ğŸ˜— ma...</td>\n",
       "      <td>Ä‘eÌ£p reÌ‰ chaÌ‚Ìt luÌ›oÌ›Ì£ng voÌ‚ cuÌ€ng xiÌ£n maÌƒi u...</td>\n",
       "      <td>ğŸ˜—</td>\n",
       "      <td>1</td>\n",
       "      <td>kissing kissing kissing kissing kissing kissin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14194</th>\n",
       "      <td>ÄeÌ£p, seÌƒ uÌ‰ng hoÌ£Ì‚ tieÌ‚Ìp nheÌ ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•...</td>\n",
       "      <td>Ä‘eÌ£p seÌƒ uÌ‰ng hoÌ£Ì‚ tieÌ‚Ìp nheÌ</td>\n",
       "      <td>ğŸ‘•</td>\n",
       "      <td>1</td>\n",
       "      <td>tshirt tshirt tshirt tshirt tshirt tshirt tshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14200</th>\n",
       "      <td>Mua veÌ‚Ì€ cho choÌ‚Ì€ng maÌ€ Ä‘eÌ‚Ì‰ con em noÌ maÌ£Ì†c...</td>\n",
       "      <td>mua veÌ‚Ì€ cho choÌ‚Ì€ng maÌ€ con em maÌ£Ì†c thuÌ›Ì‰ tr...</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1283 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_comment  \\\n",
       "4      MiÌ€nh mua 2 caÌi, moÌ£Ì‚t duÌ€ng ok. MoÌ£Ì‚t caÌi k...   \n",
       "15     Giao sai maÌ€u sai size coÌ 1 deÌp loÌ‚ng size 3...   \n",
       "30     ÄoÌ›n haÌ€ng Ä‘aÌƒ thanh toaÌn airpay roÌ‚Ì€i maÌ€ sh...   \n",
       "43     AÌo croptop freesize roÌ£Ì‚ng voÌ›Ìi miÌ€nh \\nMiÌ€n...   \n",
       "47     KhaÌ buoÌ‚Ì€n . ÄaÌ£Ì†t 2 caÌi Ä‘eÌ‚Ì€u khoÌ‚ng chaÌ£y ...   \n",
       "...                                                  ...   \n",
       "14184  KhoÌ‚ng mua hoÌ‚Ìi haÌ£Ì‚n Ä‘uÌ›Ì€ng keÌ‚u nhaÌ haha, ...   \n",
       "14186  khuyeÌ‚n xinh laÌ†Ìm luÌc miÌ€nh Ä‘aÌ£Ì†t coÌ€n Ä‘uÌ›oÌ›...   \n",
       "14193  Ä‘eÌ£p reÌ‰ chaÌ‚Ìt luÌ›oÌ›Ì£ng voÌ‚ cuÌ€ng xiÌ£n ğŸ˜—ğŸ˜—ğŸ˜— ma...   \n",
       "14194  ÄeÌ£p, seÌƒ uÌ‰ng hoÌ£Ì‚ tieÌ‚Ìp nheÌ ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•ğŸ‘•...   \n",
       "14200  Mua veÌ‚Ì€ cho choÌ‚Ì€ng maÌ€ Ä‘eÌ‚Ì‰ con em noÌ maÌ£Ì†c...   \n",
       "\n",
       "                                       normalize_comment emoji  label  \\\n",
       "4      mua caÌi moÌ£Ì‚t duÌ€ng ok moÌ£Ì‚t caÌi khoÌ‚ng chaÌ£...     ğŸ˜¢      0   \n",
       "15     giao sai maÌ€u sai size coÌ deÌp loÌ‚ng size aÌ€ ...     ğŸ¤¬      0   \n",
       "30     Ä‘oÌ›n haÌ€ng thanh toaÌn maÌ€ giao cho nguÌ›oÌ›Ì€i l...     ğŸ™„      0   \n",
       "43                                   aÌo roÌ£Ì‚ng miÌ€nh kg     ğŸ‘      0   \n",
       "47     khaÌ buoÌ‚Ì€n Ä‘aÌ£Ì†t caÌi Ä‘eÌ‚Ì€u khoÌ‚ng chaÌ£y gioÌ›...  â™€ï¸ ğŸ¤·      0   \n",
       "...                                                  ...   ...    ...   \n",
       "14184  khoÌ‚ng mua hoÌ‚Ìi haÌ£Ì‚n Ä‘uÌ›Ì€ng keÌ‚u nhaÌ haÌ€ng ...     ğŸ˜‚      1   \n",
       "14186  khuyeÌ‚n xinh laÌ†Ìm luÌc Ä‘aÌ£Ì†t coÌ€n Ä‘uÌ›oÌ›Ì£c dea...     ğŸ˜˜      1   \n",
       "14193  Ä‘eÌ£p reÌ‰ chaÌ‚Ìt luÌ›oÌ›Ì£ng voÌ‚ cuÌ€ng xiÌ£n maÌƒi u...     ğŸ˜—      1   \n",
       "14194                     Ä‘eÌ£p seÌƒ uÌ‰ng hoÌ£Ì‚ tieÌ‚Ìp nheÌ     ğŸ‘•      1   \n",
       "14200  mua veÌ‚Ì€ cho choÌ‚Ì€ng maÌ€ con em maÌ£Ì†c thuÌ›Ì‰ tr...     ğŸ˜‚      1   \n",
       "\n",
       "                                            decode_emoji  \n",
       "4                                                    cry  \n",
       "15                cursing_face cursing_face cursing_face  \n",
       "30                                             roll_eyes  \n",
       "43     thumbsup thumbsup thumbsup thumbsup thumbsup t...  \n",
       "47                                     shrug female_sign  \n",
       "...                                                  ...  \n",
       "14184                                                joy  \n",
       "14186          kissing_heart kissing_heart kissing_heart  \n",
       "14193  kissing kissing kissing kissing kissing kissin...  \n",
       "14194  tshirt tshirt tshirt tshirt tshirt tshirt tshi...  \n",
       "14200                                                joy  \n",
       "\n",
       "[1283 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "BÆ°á»›c nÃ y, ta thá»±c hiá»‡n chia `emojis` ra thÃ nh hai táº­p training data vÃ  test data vá»›i test data chiáº¿m 20% sá»‘ lÆ°á»£ng cÃ¡c observer cá»§a `emojis`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['decode_emoji'], data['label'], test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1521                                            heart_eyes\n",
       "11485    hearts hearts hearts kissing_heart kissing_hea...\n",
       "3467                                              relieved\n",
       "7740     persevere persevere persevere persevere persev...\n",
       "12870                                          heart heart\n",
       "                               ...                        \n",
       "12351                                           gift_heart\n",
       "12799                                                heart\n",
       "13056                                            hugs wink\n",
       "10730    blush blush blush blush blush blush blush blus...\n",
       "13032                                                 wink\n",
       "Name: decode_emoji, Length: 1026, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "BÃ¢y giá», chÃºng ta thá»±c hiá»‡n kÄ© thuáº­t **vectorizing text**, ta Ã¡p dá»¥ng láº§n lÆ°á»£t hai phÆ°Æ¡ng phÃ¡p lÃ  **Bag of Words** vÃ  **TF-IDF**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Bag of words\n",
    "bow_vec = CountVectorizer()\n",
    "bow_emojis = bow_vec.fit_transform(X_train)\n",
    "\n",
    "print(bow_vec.get_feature_names())\n",
    "print(bow_emojis.toarray())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['100', 'anger', 'angry', 'ballet_shoes', 'bangbang', 'birthday', 'blossom', 'blue_heart', 'blush', 'bouquet', 'bow', 'broken_heart', 'butterfly', 'cake', 'camera', 'carrot', 'cherry_blossom', 'clap', 'cloud_with_rain', 'clown_face', 'cold_face', 'cold_sweat', 'compass', 'confounded', 'confused', 'cow', 'crossed_fingers', 'cry', 'crying_cat_face', 'cursing_face', 'disappointed', 'disappointed_relieved', 'dizzy', 'dizzy_face', 'dog', 'dog2', 'dollar', 'eagle', 'expressionless', 'face_with_thermometer', 'facepalm', 'fallen_leaf', 'fearful', 'female_sign', 'fire', 'fist_left', 'floppy_disk', 'flushed', 'four_leaf_clover', 'free', 'frowning_face', 'fu', 'ghost', 'gift_heart', 'golf', 'green_heart', 'green_salad', 'grimacing', 'grin', 'grinning', 'haircut', 'hand_over_mouth', 'hatched_chick', 'heart', 'heart_eyes', 'heart_eyes_cat', 'heartbeat', 'heartpulse', 'hearts', 'heavy_check_mark', 'heavy_heart_exclamation', 'hibiscus', 'high_brightness', 'hugs', 'information_desk_person', 'innocent', 'joy', 'joy_cat', 'kiss', 'kissing', 'kissing_cat', 'kissing_closed_eyes', 'kissing_heart', 'kissing_smiling_eyes', 'knife', 'lobster', 'lollipop', 'loud_sound', 'male_sign', 'man_facepalming', 'mask', 'moneybag', 'musical_score', 'nauseated_face', 'nerd_face', 'neutral_face', 'no_good', 'no_mouth', 'octopus', 'ok_hand', 'ok_man', 'ok_person', 'ok_woman', 'older_man', 'open_mouth', 'partying_face', 'pensive', 'persevere', 'pig_nose', 'pleading_face', 'point_left', 'point_right', 'potted_plant', 'pout', 'pray', 'purple_heart', 'pushpin', 'raised_eyebrow', 'raising_hand_woman', 'red_circle', 'relaxed', 'relieved', 'revolving_hearts', 'ribbon', 'rice', 'rice_ball', 'rofl', 'roll_eyes', 'satisfied', 'scream', 'see_no_evil', 'shamrock', 'shit', 'shrug', 'skull_and_crossbones', 'sleeping', 'sleepy', 'slightly_frowning_face', 'slightly_smiling_face', 'smile', 'smiley', 'smiley_cat', 'smiling_face_with_tear', 'smiling_face_with_three_hearts', 'smiling_imp', 'smirk', 'sneezing_face', 'sob', 'soon', 'sos', 'spaghetti', 'sparkles', 'sparkling_heart', 'speak_no_evil', 'squid', 'star', 'star2', 'star_struck', 'strawberry', 'stuck_out_tongue', 'stuck_out_tongue_closed_eyes', 'stuck_out_tongue_winking_eye', 'sun_behind_rain_cloud', 'sun_with_face', 'sunglasses', 'sweat', 'sweat_drops', 'sweat_smile', 'tada', 'telephone_receiver', 'thinking', 'thumbsdown', 'thumbsup', 'tired_face', 'triumph', 'tshirt', 'two_hearts', 'unamused', 'upside_down_face', 'vietnam', 'vomiting_face', 'walking', 'wave', 'weary', 'white_check_mark', 'white_heart', 'wink', 'woman_facepalming', 'woman_shrugging', 'woozy_face', 'worried', 'yellow_heart', 'yum', 'zany_face', 'zipper_mouth_face']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_emojis = tfidf_vec.fit_transform(X_train)\n",
    "\n",
    "print(tfidf_emojis.toarray())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "X_vectorizers = [\n",
    "    ('Bag of Words', bow_emojis),\n",
    "    ('TF-IDF', tfidf_emojis)\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "BÃ¢y giá» ta liá»‡t kÃª táº¥t cáº£ cÃ¡c model mÃ  ta cÃ³ thá»ƒ ap dá»¥ng vÃ o bÃ i toÃ¡n Binary Classification vÃ o má»™t list object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "lst_models = [\n",
    "    ('Logistic Regression - [solver: lbfgs]', LogisticRegression(solver='lbfgs')),\n",
    "    ('Logistic Regression - [solver: liblinear]', LogisticRegression(solver='liblinear')),\n",
    "    ('Logistic Regression - [solver: newton-cg]', LogisticRegression(solver='newton-cg')),\n",
    "    ('KNN - [n_neighbors: 2]', KNeighborsClassifier(n_neighbors=2)),\n",
    "    ('KNN - [n_neighbors: 3]', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('SVC - [kernel: linear]', SVC(kernel='linear', random_state=42)),\n",
    "    ('SVC - [kernel: poly]', SVC(kernel='poly', random_state=42)),\n",
    "    ('SVC - [kernel: rbf]', SVC(kernel='rbf', random_state=42)),\n",
    "    ('SVC - [kernel: sigmoid]', SVC(kernel='sigmoid', random_state=42)),\n",
    "    ('Bernoulli', BernoulliNB()),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('AdaBoost', AdaBoostClassifier(base_estimator=RandomForestClassifier(random_state=42), random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='mlogloss'))\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def train(lst_models, X_vectorizer, y):\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    res_table = []\n",
    "    for vec_name, X in X_vectorizer:\n",
    "        for mdl_name, model in lst_models:\n",
    "            cv_res = cross_validate(model, X, y, cv=cv, return_train_score=True, scoring=['accuracy', 'roc_auc'])\n",
    "            res_table.append([vec_name, mdl_name,\n",
    "                              cv_res['train_accuracy'].mean(),\n",
    "                              cv_res['test_accuracy'].mean(),\n",
    "                              np.abs(cv_res['train_accuracy'].mean() - cv_res['test_accuracy'].mean()),\n",
    "                              cv_res['train_accuracy'].std(),\n",
    "                              cv_res['test_accuracy'].std(),\n",
    "                              cv_res['train_roc_auc'].mean(),\n",
    "                              cv_res['test_roc_auc'].mean(),\n",
    "                              np.abs(cv_res['train_roc_auc'].mean() - cv_res['test_roc_auc'].mean()),\n",
    "                              cv_res['train_roc_auc'].std(),\n",
    "                              cv_res['test_roc_auc'].std(),\n",
    "                              cv_res['fit_time'].mean()\n",
    "            ])\n",
    "    \n",
    "    res_table = pd.DataFrame(res_table, columns=['vectorizer', 'model', 'train_acc', 'test_acc', 'diff_acc',\n",
    "                                                 'train_acc_std', 'test_acc_std', 'train_roc_auc', 'test_roc_auc',\n",
    "                                                 'diff_roc_auc', 'train_roc_auc_std', 'test_roc_auc_std', 'fit_time'])\n",
    "    res_table.sort_values(by=['test_acc', 'test_roc_auc'], ascending=False, inplace=True)\n",
    "    return res_table.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "res_models = train(lst_models, X_vectorizers, y_train)\n",
    "\n",
    "res_models"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>diff_acc</th>\n",
       "      <th>train_acc_std</th>\n",
       "      <th>test_acc_std</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>diff_roc_auc</th>\n",
       "      <th>train_roc_auc_std</th>\n",
       "      <th>test_roc_auc_std</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVC - [kernel: rbf]</td>\n",
       "      <td>0.893546</td>\n",
       "      <td>0.851875</td>\n",
       "      <td>0.041671</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.038218</td>\n",
       "      <td>0.905123</td>\n",
       "      <td>0.873282</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.016902</td>\n",
       "      <td>0.033987</td>\n",
       "      <td>0.014874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Logistic Regression - [solver: liblinear]</td>\n",
       "      <td>0.890189</td>\n",
       "      <td>0.850866</td>\n",
       "      <td>0.039322</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.930616</td>\n",
       "      <td>0.903272</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.036520</td>\n",
       "      <td>0.001866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Logistic Regression - [solver: lbfgs]</td>\n",
       "      <td>0.890189</td>\n",
       "      <td>0.850866</td>\n",
       "      <td>0.039322</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.930633</td>\n",
       "      <td>0.903188</td>\n",
       "      <td>0.027445</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>0.012641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Logistic Regression - [solver: newton-cg]</td>\n",
       "      <td>0.890189</td>\n",
       "      <td>0.850866</td>\n",
       "      <td>0.039322</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.930635</td>\n",
       "      <td>0.903188</td>\n",
       "      <td>0.027447</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>0.017942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.900802</td>\n",
       "      <td>0.847944</td>\n",
       "      <td>0.052858</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.045535</td>\n",
       "      <td>0.957656</td>\n",
       "      <td>0.897262</td>\n",
       "      <td>0.060395</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.209228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVC - [kernel: poly]</td>\n",
       "      <td>0.896578</td>\n",
       "      <td>0.847002</td>\n",
       "      <td>0.049576</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.039243</td>\n",
       "      <td>0.905438</td>\n",
       "      <td>0.854690</td>\n",
       "      <td>0.050747</td>\n",
       "      <td>0.006277</td>\n",
       "      <td>0.044771</td>\n",
       "      <td>0.014134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.882933</td>\n",
       "      <td>0.845012</td>\n",
       "      <td>0.037920</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.034061</td>\n",
       "      <td>0.937264</td>\n",
       "      <td>0.898412</td>\n",
       "      <td>0.038853</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.032872</td>\n",
       "      <td>0.001586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.882933</td>\n",
       "      <td>0.845012</td>\n",
       "      <td>0.037920</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.034061</td>\n",
       "      <td>0.937264</td>\n",
       "      <td>0.898412</td>\n",
       "      <td>0.038853</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.032872</td>\n",
       "      <td>0.001950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVC - [kernel: sigmoid]</td>\n",
       "      <td>0.881200</td>\n",
       "      <td>0.844080</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>0.037964</td>\n",
       "      <td>0.908762</td>\n",
       "      <td>0.881717</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>0.012880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVC - [kernel: linear]</td>\n",
       "      <td>0.887698</td>\n",
       "      <td>0.843118</td>\n",
       "      <td>0.044580</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.036253</td>\n",
       "      <td>0.908829</td>\n",
       "      <td>0.870945</td>\n",
       "      <td>0.037883</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.034686</td>\n",
       "      <td>0.010844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.900802</td>\n",
       "      <td>0.842119</td>\n",
       "      <td>0.058683</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.045126</td>\n",
       "      <td>0.962721</td>\n",
       "      <td>0.871233</td>\n",
       "      <td>0.091488</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>12.711624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>KNN - [n_neighbors: 3]</td>\n",
       "      <td>0.873727</td>\n",
       "      <td>0.834304</td>\n",
       "      <td>0.039423</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>0.048597</td>\n",
       "      <td>0.906049</td>\n",
       "      <td>0.850616</td>\n",
       "      <td>0.055433</td>\n",
       "      <td>0.005915</td>\n",
       "      <td>0.048967</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.833343</td>\n",
       "      <td>0.094857</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.043493</td>\n",
       "      <td>0.975427</td>\n",
       "      <td>0.878510</td>\n",
       "      <td>0.096917</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.029488</td>\n",
       "      <td>0.202526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Logistic Regression - [solver: newton-cg]</td>\n",
       "      <td>0.880982</td>\n",
       "      <td>0.827489</td>\n",
       "      <td>0.053493</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.935250</td>\n",
       "      <td>0.884170</td>\n",
       "      <td>0.051080</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.036731</td>\n",
       "      <td>0.028815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Logistic Regression - [solver: lbfgs]</td>\n",
       "      <td>0.880982</td>\n",
       "      <td>0.827489</td>\n",
       "      <td>0.053493</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.935252</td>\n",
       "      <td>0.884128</td>\n",
       "      <td>0.051124</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.036914</td>\n",
       "      <td>0.023275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Logistic Regression - [solver: liblinear]</td>\n",
       "      <td>0.880765</td>\n",
       "      <td>0.826518</td>\n",
       "      <td>0.054247</td>\n",
       "      <td>0.006240</td>\n",
       "      <td>0.031986</td>\n",
       "      <td>0.935249</td>\n",
       "      <td>0.884170</td>\n",
       "      <td>0.051080</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.036731</td>\n",
       "      <td>0.002355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.826518</td>\n",
       "      <td>0.101682</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.981883</td>\n",
       "      <td>0.857981</td>\n",
       "      <td>0.123902</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.032441</td>\n",
       "      <td>12.932513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>SVC - [kernel: linear]</td>\n",
       "      <td>0.884988</td>\n",
       "      <td>0.820741</td>\n",
       "      <td>0.064248</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>0.027824</td>\n",
       "      <td>0.925946</td>\n",
       "      <td>0.876632</td>\n",
       "      <td>0.049314</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.017519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.854992</td>\n",
       "      <td>0.818685</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.046203</td>\n",
       "      <td>0.920824</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.031060</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.040109</td>\n",
       "      <td>0.468371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>KNN - [n_neighbors: 3]</td>\n",
       "      <td>0.898744</td>\n",
       "      <td>0.808928</td>\n",
       "      <td>0.089816</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.026856</td>\n",
       "      <td>0.935404</td>\n",
       "      <td>0.859903</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.027268</td>\n",
       "      <td>0.001050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.861815</td>\n",
       "      <td>0.803055</td>\n",
       "      <td>0.058760</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.046481</td>\n",
       "      <td>0.930295</td>\n",
       "      <td>0.879968</td>\n",
       "      <td>0.050327</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.403061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>KNN - [n_neighbors: 2]</td>\n",
       "      <td>0.814702</td>\n",
       "      <td>0.774910</td>\n",
       "      <td>0.039793</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.042470</td>\n",
       "      <td>0.899022</td>\n",
       "      <td>0.835368</td>\n",
       "      <td>0.063654</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.001247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>KNN - [n_neighbors: 2]</td>\n",
       "      <td>0.823909</td>\n",
       "      <td>0.746678</td>\n",
       "      <td>0.077231</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>0.033918</td>\n",
       "      <td>0.926534</td>\n",
       "      <td>0.816691</td>\n",
       "      <td>0.109842</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>0.033904</td>\n",
       "      <td>0.001154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>SVC - [kernel: rbf]</td>\n",
       "      <td>0.755578</td>\n",
       "      <td>0.714382</td>\n",
       "      <td>0.041195</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.030526</td>\n",
       "      <td>0.919260</td>\n",
       "      <td>0.874206</td>\n",
       "      <td>0.045053</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.035317</td>\n",
       "      <td>0.019723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>SVC - [kernel: sigmoid]</td>\n",
       "      <td>0.689516</td>\n",
       "      <td>0.672501</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.022026</td>\n",
       "      <td>0.878113</td>\n",
       "      <td>0.847238</td>\n",
       "      <td>0.030875</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.016986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>SVC - [kernel: poly]</td>\n",
       "      <td>0.672623</td>\n",
       "      <td>0.660832</td>\n",
       "      <td>0.011791</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.822122</td>\n",
       "      <td>0.070585</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.045143</td>\n",
       "      <td>0.023206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      vectorizer                                      model  train_acc  \\\n",
       "0         TF-IDF                        SVC - [kernel: rbf]   0.893546   \n",
       "1         TF-IDF  Logistic Regression - [solver: liblinear]   0.890189   \n",
       "2         TF-IDF      Logistic Regression - [solver: lbfgs]   0.890189   \n",
       "3         TF-IDF  Logistic Regression - [solver: newton-cg]   0.890189   \n",
       "4         TF-IDF                              Random Forest   0.900802   \n",
       "5         TF-IDF                       SVC - [kernel: poly]   0.896578   \n",
       "6   Bag of Words                                  Bernoulli   0.882933   \n",
       "7         TF-IDF                                  Bernoulli   0.882933   \n",
       "8         TF-IDF                    SVC - [kernel: sigmoid]   0.881200   \n",
       "9         TF-IDF                     SVC - [kernel: linear]   0.887698   \n",
       "10        TF-IDF                                   AdaBoost   0.900802   \n",
       "11        TF-IDF                     KNN - [n_neighbors: 3]   0.873727   \n",
       "12  Bag of Words                              Random Forest   0.928200   \n",
       "13  Bag of Words  Logistic Regression - [solver: newton-cg]   0.880982   \n",
       "14  Bag of Words      Logistic Regression - [solver: lbfgs]   0.880982   \n",
       "15  Bag of Words  Logistic Regression - [solver: liblinear]   0.880765   \n",
       "16  Bag of Words                                   AdaBoost   0.928200   \n",
       "17  Bag of Words                     SVC - [kernel: linear]   0.884988   \n",
       "18        TF-IDF                                    XGBoost   0.854992   \n",
       "19  Bag of Words                     KNN - [n_neighbors: 3]   0.898744   \n",
       "20  Bag of Words                                    XGBoost   0.861815   \n",
       "21        TF-IDF                     KNN - [n_neighbors: 2]   0.814702   \n",
       "22  Bag of Words                     KNN - [n_neighbors: 2]   0.823909   \n",
       "23  Bag of Words                        SVC - [kernel: rbf]   0.755578   \n",
       "24  Bag of Words                    SVC - [kernel: sigmoid]   0.689516   \n",
       "25  Bag of Words                       SVC - [kernel: poly]   0.672623   \n",
       "\n",
       "    test_acc  diff_acc  train_acc_std  test_acc_std  train_roc_auc  \\\n",
       "0   0.851875  0.041671       0.003487      0.038218       0.905123   \n",
       "1   0.850866  0.039322       0.004251      0.033072       0.930616   \n",
       "2   0.850866  0.039322       0.004251      0.033072       0.930633   \n",
       "3   0.850866  0.039322       0.004251      0.033072       0.930635   \n",
       "4   0.847944  0.052858       0.003933      0.045535       0.957656   \n",
       "5   0.847002  0.049576       0.003524      0.039243       0.905438   \n",
       "6   0.845012  0.037920       0.003072      0.034061       0.937264   \n",
       "7   0.845012  0.037920       0.003072      0.034061       0.937264   \n",
       "8   0.844080  0.037121       0.003237      0.037964       0.908762   \n",
       "9   0.843118  0.044580       0.003094      0.036253       0.908829   \n",
       "10  0.842119  0.058683       0.003933      0.045126       0.962721   \n",
       "11  0.834304  0.039423       0.004453      0.048597       0.906049   \n",
       "12  0.833343  0.094857       0.003246      0.043493       0.975427   \n",
       "13  0.827489  0.053493       0.006239      0.031486       0.935250   \n",
       "14  0.827489  0.053493       0.006239      0.031486       0.935252   \n",
       "15  0.826518  0.054247       0.006240      0.031986       0.935249   \n",
       "16  0.826518  0.101682       0.003246      0.037255       0.981883   \n",
       "17  0.820741  0.064248       0.007880      0.027824       0.925946   \n",
       "18  0.818685  0.036308       0.006688      0.046203       0.920824   \n",
       "19  0.808928  0.089816       0.003685      0.026856       0.935404   \n",
       "20  0.803055  0.058760       0.005243      0.046481       0.930295   \n",
       "21  0.774910  0.039793       0.016428      0.042470       0.899022   \n",
       "22  0.746678  0.077231       0.018303      0.033918       0.926534   \n",
       "23  0.714382  0.041195       0.005464      0.030526       0.919260   \n",
       "24  0.672501  0.017015       0.003921      0.022026       0.878113   \n",
       "25  0.660832  0.011791       0.001305      0.009961       0.892707   \n",
       "\n",
       "    test_roc_auc  diff_roc_auc  train_roc_auc_std  test_roc_auc_std   fit_time  \n",
       "0       0.873282      0.031841           0.016902          0.033987   0.014874  \n",
       "1       0.903272      0.027344           0.003066          0.036520   0.001866  \n",
       "2       0.903188      0.027445           0.003078          0.036723   0.012641  \n",
       "3       0.903188      0.027447           0.003078          0.036723   0.017942  \n",
       "4       0.897262      0.060395           0.002273          0.035206   0.209228  \n",
       "5       0.854690      0.050747           0.006277          0.044771   0.014134  \n",
       "6       0.898412      0.038853           0.002624          0.032872   0.001586  \n",
       "7       0.898412      0.038853           0.002624          0.032872   0.001950  \n",
       "8       0.881717      0.027045           0.010184          0.043808   0.012880  \n",
       "9       0.870945      0.037883           0.011004          0.034686   0.010844  \n",
       "10      0.871233      0.091488           0.002259          0.044248  12.711624  \n",
       "11      0.850616      0.055433           0.005915          0.048967   0.001244  \n",
       "12      0.878510      0.096917           0.001451          0.029488   0.202526  \n",
       "13      0.884170      0.051080           0.003135          0.036731   0.028815  \n",
       "14      0.884128      0.051124           0.003135          0.036914   0.023275  \n",
       "15      0.884170      0.051080           0.003132          0.036731   0.002355  \n",
       "16      0.857981      0.123902           0.001446          0.032441  12.932513  \n",
       "17      0.876632      0.049314           0.003860          0.037206   0.017519  \n",
       "18      0.889764      0.031060           0.003258          0.040109   0.468371  \n",
       "19      0.859903      0.075500           0.003627          0.027268   0.001050  \n",
       "20      0.879968      0.050327           0.003141          0.031373   0.403061  \n",
       "21      0.835368      0.063654           0.007371          0.052375   0.001247  \n",
       "22      0.816691      0.109842           0.005107          0.033904   0.001154  \n",
       "23      0.874206      0.045053           0.002626          0.035317   0.019723  \n",
       "24      0.847238      0.030875           0.010735          0.031568   0.016986  \n",
       "25      0.822122      0.070585           0.009763          0.045143   0.023206  "
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Nháº­n xÃ©t**:\n",
    "> * Báº£ng trÃªn lÃ  thá»‘ng kÃª vá» cÃ¡c chá»‰ sá»‘ **accuracy** vÃ  **ROG-AUC** cá»§a 26 model Ä‘Æ°á»£c **sáº¯p xáº¿p giáº£m dáº§n** dá»±a trÃªn **valuation accuracy** _(trong sklearn Ä‘á»‹nh nghÄ©a lÃ  `test_accuracy`)_ vÃ  **valuation ROC-AUC** _(trong sklearn Ä‘á»‹nh nghÄ©a lÃ  `test_roc_auc`)_.\n",
    "> * CÃ³ thá»ƒ tháº¥y, sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c metric trÃªn **train** vÃ  **valuation** lÃ  khÃ´ng quÃ¡ lá»›n _(xem vÃ o `diff_acc` vÃ  `diff_roc_auc`)_, trung bÃ¬nh cÃ³ thá»ƒ tháº¥y chÃºng dao Ä‘á»™ng dÆ°á»›i 5%. $\\Rightarrow$ CÃ¡c model cá»§a chÃºng ta khÃ´ng bá»‹ overfitting.\n",
    "> * Sá»‘ Ã­t á»Ÿ cÃ¡c model KNN vÃ  AdaBoost cÃ³ cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ cho káº¿t quáº£ khÃ´ng tá»‘t vÃ¬ chÃºng dao Ä‘á»™ng xung quanh 10% $\\Rightarrow$ Hai thuáº­t toÃ¡n nÃ y khÃ´ng phÃ¹ há»£p vá»›i bÃ i toÃ¡n Emoji sentiment analysis trÃªn dataset cá»§a chÃºng ta.\n",
    "> * PhÆ°Æ¡ng phÃ¡p text vectorizing **TF-IDF** hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n **Bag of words**, cÃ³ thá»ƒ tháº¥y á»Ÿ 10 model cho hiá»‡u suáº¥t tá»‘t nháº¥t thÃ¬ phÆ°Æ¡ng phÃ¡p **TF-IDF** chiáº¿m lÄ©nh toÃ n bá»™ trong viá»‡c Ä‘Æ°a ra má»™t cÃ¡ch tá»‘i Æ°u hÃ³a Ä‘áº§u vÃ o tá»‘t trÃªn cÃ¡c model.\n",
    "> * Vá» evaluation, cáº£ **train/valuation accuracy** vÃ  **train/valuation ROC-AUC** khÃ¡ tá»‘t, luÃ´n thuá»™c pháº¡m vi $[80, 100]\\%$ á»Ÿ 10 model Ä‘áº§u tiÃªn. \n",
    "> * NhÆ° váº­y, cÃ¡c model cá»§a chÃºng ta khÃ´ng bá»‹ overfitting cÅ©ng nhÆ° bias vÃ  variance error. Ta cÃ³ thá»ƒ chá»n ra 5 model Ä‘áº§u tiÃªn Ä‘á»ƒ tiáº¿n hÃ nh Hyperparameters Optimization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}