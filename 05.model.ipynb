{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import modules.eda as Detective\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import emojis\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def expandEmojisDecode(pcomment: str):\n",
    "    expand_emojis = ''\n",
    "    for e in emojis.get(pcomment):\n",
    "        amount = pcomment.count(e)\n",
    "        expand_emojis += (f\"{emojis.decode(e)[1:-1]} \")*amount\n",
    "        \n",
    "    return expand_emojis.strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "reviews = pd.read_csv(\"./data/normalize_reviews.csv\").fillna(\"\")\n",
    "reviews = reviews[['raw_comment', 'normalize_comment', 'emoji', 'label']]\n",
    "\n",
    "reviews.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>normalize_comment</th>\n",
       "      <th>emoji</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Giao hàng kh đúng cần phê bình hjjjjjhhd...</td>\n",
       "      <td>giao hàng không đúng cần phê bình</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chất lượng sản phẩm tạm được. Giao...</td>\n",
       "      <td>chất lượng sản phẩm tạm được giao ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ko có lắc tay như hình</td>\n",
       "      <td>không có lắc tay như hình</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Giao hàng lâu. Bảo có lắc tay mà k thâ...</td>\n",
       "      <td>giao hàng lâu bảo có lắc tay mà không ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mình mua 2 cái, một dùng ok. Một cái k...</td>\n",
       "      <td>mua cái một dùng ok một cái không chạ...</td>\n",
       "      <td>😢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         raw_comment  \\\n",
       "0  Giao hàng kh đúng cần phê bình hjjjjjhhd...   \n",
       "1  Chất lượng sản phẩm tạm được. Giao...   \n",
       "2                        Ko có lắc tay như hình   \n",
       "3  Giao hàng lâu. Bảo có lắc tay mà k thâ...   \n",
       "4  Mình mua 2 cái, một dùng ok. Một cái k...   \n",
       "\n",
       "                                   normalize_comment emoji  label  \n",
       "0           giao hàng không đúng cần phê bình            0  \n",
       "1  chất lượng sản phẩm tạm được giao ...            0  \n",
       "2                    không có lắc tay như hình            0  \n",
       "3  giao hàng lâu bảo có lắc tay mà không ...            0  \n",
       "4  mua cái một dùng ok một cái không chạ...     😢      0  "
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hiện tại, dataset chúng ta có hai feature là `normalize_comment` và `emoji`, chúng ta có thể xây dựng một sentiment analysis classifier bằng hai cách:\n",
    "  * **Cách 1**: Chúng ta có thể đem feature `emoji` để thêm vào phần đầu hoặc cuối của feature `normalize_comment`. Sau đó tiến hành đào tạo một model duy nhất với input là feature `normalize_comment` đã được thêm vào feature `emoji`.\n",
    "    * Ưu điểm: Quá trình đào tạo model đỡ tốn thời gian hơn do ta chỉ thực hiện build một model duy nhất.\n",
    "    * Nhược điểm:\n",
    "      * Có khả năng làm suy giảm sức mạnh của model, nhiều người họ còn xây dưng riêng một **emoji classifier** bởi vì theo họ nhiều khi emoji đó mang lại ý nghĩa thậm chí còn cao hơn so với ngữ nghĩa của comment đó. Trong khi việc đào tạo một emoji classifier đỡ vất vã hơn.\n",
    "      * Ta phải tốn thời gian vào việc xây dựng thêm một model riêng sau đó lại tổng hợp kết quả của emoji classifier model này với sentiment classifier model ban đầu.\n",
    "  * **Cách 2**: Ta có thể xây dựng hai model riêng biệt, một sentiment analysis model đảm nhận nhiệm vụ phân lớp cho câu chữ và một emoji analysis model đảm nhận nhiệm vụ phân lớp trên emoji. Sau đó ta kết hợp hai model này để cho ra kết quả phân lớp cuối cùng.\n",
    "    * Ưu điểm: Kết quả phân lớp cho ra có khả năng chính xác hơn, ta có thể linh hoạt trong việc thiếp lập trọng số để ưu tiên giá trị của model nào mang lại lợi ích cao hơn cho kết quả ở bước kết hợp hai model lại để dự đoán.\n",
    "    * Nhược điểm:\n",
    "      * Vất vả hơn về mặt tiền xử lí dữ liệu, tốn thêm thời gian phân tích và thiết kế thêm một emoji classifier.\n",
    "      * Phần lớn trong dataset hiện tại, số comment chứa emoji không nhiều. Như project 2 ta đã thấy toàn bộ dataset của ta chỉ khoảng 1300 comment là có chứa emoji kèm theo, ta có thể khắc phục bằng một trong những cách:\n",
    "        * Crawl thêm data, nhưng việc này không khả khi. Ta đã đề cập rằng số comment chứa emoji rất ít trong một dataset nên việc ta hi sinh thời gian chỉ để trích xuất một phần nhỏ trong một dataset cực lớn sau khi crawl về có thể phí phạm.\n",
    "        * Sử dụng kĩ thuật **Data Upsampler** với package của IBM `from imblearn.over_sampling import RandomOverSampler`.\n",
    "\n",
    "    $\\Rightarrow$ Hướng giải quyết: Quả thật ta có khoảng 1300 observer thì là một con số không quá ít cũng không quá nhiều, số lượng emoji chứa trong một comment cũng không nhiều (không tính các duplicate emoji). Bài toán của chúng ta đơn thuần chỉ là phân tích các emoji ra hai class là negative và positive. Hãy cùng nhìn lại một dataset nổi tiếng khác là **Iris**, ta có 3 class ở target variable và 150 observe. Vậy ta vẫn có thể build một classifier model đủ tốt nếu input của ta cũng **đủ tốt**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Emoji sentiment analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Như vậy chiến lược đào tạo một Emoji sentiment analysis của chúng ta sẽ như sau:\n",
    "  * Bước 1: Tiến hành lấy toàn bộ các emoji trong các comment và lưu trong cấu trúc dữ liệu `set`, mục tiêu ta cần biết có bao nhiêu unique emoji trong dataset của chúng ta.\n",
    "  * Bước 2: Tạo 2 dictionary, một dictionary có key là emoji và value là index của emoji đó trong `set`, dictionary thứ hai có key là index của emoji trong `set` và value là emoji.\n",
    "  * Bước 3: Giả sử có $n$ unique emoji trong dataset, ta sẽ chỉ quan tâm đến các comment chứa emoji, và với từng comment như vậy ta tạo một vector $n$ phần tử sau đó áp dụng phương pháp **Bag of words** lên vector này.\n",
    "  * Bước 4: Xem xét $n$ có lớn không, nếu quá lớn thì có thể áp dụng các phương pháp giảm chiều dữ liệu.\n",
    "  * Bước 5: Dùng các vector $n$ phần tử + label của chúng sau khi đã qua bước 4 để đào tạo một classifier model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bây giờ ta sẽ tiến hành lọc ra nhựng comment mà có chứa emoji dựa vào feature `emoji`. Sau đó lưu các observe này vào biến `emojis`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data = reviews[reviews['emoji'] != \"\"]\n",
    "\n",
    "data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>normalize_comment</th>\n",
       "      <th>emoji</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mình mua 2 cái, một dùng ok. Một cái k...</td>\n",
       "      <td>mua cái một dùng ok một cái không chạ...</td>\n",
       "      <td>😢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Giao sai màu sai size có 1 dép lông size 3...</td>\n",
       "      <td>giao sai màu sai size có dép lông size à ...</td>\n",
       "      <td>🤬</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Đơn hàng đã thanh toán airpay rồi mà sh...</td>\n",
       "      <td>đơn hàng thanh toán mà giao cho người l...</td>\n",
       "      <td>🙄</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Áo croptop freesize rộng với mình \\nMìn...</td>\n",
       "      <td>áo rộng mình kg</td>\n",
       "      <td>👍</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Khá buồn . Đặt 2 cái đều không chạy ...</td>\n",
       "      <td>khá buồn đặt cái đều không chạy giơ...</td>\n",
       "      <td>♀️ 🤷</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>Không mua hối hận đừng kêu nhá haha, ...</td>\n",
       "      <td>không mua hối hận đừng kêu nhá hàng ...</td>\n",
       "      <td>😂</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>khuyên xinh lắm lúc mình đặt còn đươ...</td>\n",
       "      <td>khuyên xinh lắm lúc đặt còn được dea...</td>\n",
       "      <td>😘</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14193</th>\n",
       "      <td>đẹp rẻ chất lượng vô cùng xịn 😗😗😗 ma...</td>\n",
       "      <td>đẹp rẻ chất lượng vô cùng xịn mãi u...</td>\n",
       "      <td>😗</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14194</th>\n",
       "      <td>Đẹp, sẽ ủng hộ tiếp nhé 👕👕👕👕👕👕👕👕👕👕👕👕👕👕...</td>\n",
       "      <td>đẹp sẽ ủng hộ tiếp nhé</td>\n",
       "      <td>👕</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14200</th>\n",
       "      <td>Mua về cho chồng mà để con em nó mặc...</td>\n",
       "      <td>mua về cho chồng mà con em mặc thử tr...</td>\n",
       "      <td>😂</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1283 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_comment  \\\n",
       "4      Mình mua 2 cái, một dùng ok. Một cái k...   \n",
       "15     Giao sai màu sai size có 1 dép lông size 3...   \n",
       "30     Đơn hàng đã thanh toán airpay rồi mà sh...   \n",
       "43     Áo croptop freesize rộng với mình \\nMìn...   \n",
       "47     Khá buồn . Đặt 2 cái đều không chạy ...   \n",
       "...                                                  ...   \n",
       "14184  Không mua hối hận đừng kêu nhá haha, ...   \n",
       "14186  khuyên xinh lắm lúc mình đặt còn đươ...   \n",
       "14193  đẹp rẻ chất lượng vô cùng xịn 😗😗😗 ma...   \n",
       "14194  Đẹp, sẽ ủng hộ tiếp nhé 👕👕👕👕👕👕👕👕👕👕👕👕👕👕...   \n",
       "14200  Mua về cho chồng mà để con em nó mặc...   \n",
       "\n",
       "                                       normalize_comment emoji  label  \n",
       "4      mua cái một dùng ok một cái không chạ...     😢      0  \n",
       "15     giao sai màu sai size có dép lông size à ...     🤬      0  \n",
       "30     đơn hàng thanh toán mà giao cho người l...     🙄      0  \n",
       "43                                   áo rộng mình kg     👍      0  \n",
       "47     khá buồn đặt cái đều không chạy giơ...  ♀️ 🤷      0  \n",
       "...                                                  ...   ...    ...  \n",
       "14184  không mua hối hận đừng kêu nhá hàng ...     😂      1  \n",
       "14186  khuyên xinh lắm lúc đặt còn được dea...     😘      1  \n",
       "14193  đẹp rẻ chất lượng vô cùng xịn mãi u...     😗      1  \n",
       "14194                     đẹp sẽ ủng hộ tiếp nhé     👕      1  \n",
       "14200  mua về cho chồng mà con em mặc thử tr...     😂      1  \n",
       "\n",
       "[1283 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Package **emojis** chỉ hỗ trợ chức năng tách emoji ra khỏi text, nó không hỗ trợ thống kê là một emoji xuất hiện bao nhiêu lần trong text đó. Bây giờ ta sẽ tách các emoji trong comment ra nhưng vẫn bảo toàn về mặt số lượng ban đầu của nó trong feature `raw_comment` và lưu vào feature `emoji`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data['decode_emoji'] = data['raw_comment'].apply(lambda cmt: expandEmojisDecode(cmt))\n",
    "\n",
    "data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>normalize_comment</th>\n",
       "      <th>emoji</th>\n",
       "      <th>label</th>\n",
       "      <th>decode_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mình mua 2 cái, một dùng ok. Một cái k...</td>\n",
       "      <td>mua cái một dùng ok một cái không chạ...</td>\n",
       "      <td>😢</td>\n",
       "      <td>0</td>\n",
       "      <td>cry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Giao sai màu sai size có 1 dép lông size 3...</td>\n",
       "      <td>giao sai màu sai size có dép lông size à ...</td>\n",
       "      <td>🤬</td>\n",
       "      <td>0</td>\n",
       "      <td>cursing_face cursing_face cursing_face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Đơn hàng đã thanh toán airpay rồi mà sh...</td>\n",
       "      <td>đơn hàng thanh toán mà giao cho người l...</td>\n",
       "      <td>🙄</td>\n",
       "      <td>0</td>\n",
       "      <td>roll_eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Áo croptop freesize rộng với mình \\nMìn...</td>\n",
       "      <td>áo rộng mình kg</td>\n",
       "      <td>👍</td>\n",
       "      <td>0</td>\n",
       "      <td>thumbsup thumbsup thumbsup thumbsup thumbsup t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Khá buồn . Đặt 2 cái đều không chạy ...</td>\n",
       "      <td>khá buồn đặt cái đều không chạy giơ...</td>\n",
       "      <td>♀️ 🤷</td>\n",
       "      <td>0</td>\n",
       "      <td>shrug female_sign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>Không mua hối hận đừng kêu nhá haha, ...</td>\n",
       "      <td>không mua hối hận đừng kêu nhá hàng ...</td>\n",
       "      <td>😂</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>khuyên xinh lắm lúc mình đặt còn đươ...</td>\n",
       "      <td>khuyên xinh lắm lúc đặt còn được dea...</td>\n",
       "      <td>😘</td>\n",
       "      <td>1</td>\n",
       "      <td>kissing_heart kissing_heart kissing_heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14193</th>\n",
       "      <td>đẹp rẻ chất lượng vô cùng xịn 😗😗😗 ma...</td>\n",
       "      <td>đẹp rẻ chất lượng vô cùng xịn mãi u...</td>\n",
       "      <td>😗</td>\n",
       "      <td>1</td>\n",
       "      <td>kissing kissing kissing kissing kissing kissin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14194</th>\n",
       "      <td>Đẹp, sẽ ủng hộ tiếp nhé 👕👕👕👕👕👕👕👕👕👕👕👕👕👕...</td>\n",
       "      <td>đẹp sẽ ủng hộ tiếp nhé</td>\n",
       "      <td>👕</td>\n",
       "      <td>1</td>\n",
       "      <td>tshirt tshirt tshirt tshirt tshirt tshirt tshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14200</th>\n",
       "      <td>Mua về cho chồng mà để con em nó mặc...</td>\n",
       "      <td>mua về cho chồng mà con em mặc thử tr...</td>\n",
       "      <td>😂</td>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1283 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_comment  \\\n",
       "4      Mình mua 2 cái, một dùng ok. Một cái k...   \n",
       "15     Giao sai màu sai size có 1 dép lông size 3...   \n",
       "30     Đơn hàng đã thanh toán airpay rồi mà sh...   \n",
       "43     Áo croptop freesize rộng với mình \\nMìn...   \n",
       "47     Khá buồn . Đặt 2 cái đều không chạy ...   \n",
       "...                                                  ...   \n",
       "14184  Không mua hối hận đừng kêu nhá haha, ...   \n",
       "14186  khuyên xinh lắm lúc mình đặt còn đươ...   \n",
       "14193  đẹp rẻ chất lượng vô cùng xịn 😗😗😗 ma...   \n",
       "14194  Đẹp, sẽ ủng hộ tiếp nhé 👕👕👕👕👕👕👕👕👕👕👕👕👕👕...   \n",
       "14200  Mua về cho chồng mà để con em nó mặc...   \n",
       "\n",
       "                                       normalize_comment emoji  label  \\\n",
       "4      mua cái một dùng ok một cái không chạ...     😢      0   \n",
       "15     giao sai màu sai size có dép lông size à ...     🤬      0   \n",
       "30     đơn hàng thanh toán mà giao cho người l...     🙄      0   \n",
       "43                                   áo rộng mình kg     👍      0   \n",
       "47     khá buồn đặt cái đều không chạy giơ...  ♀️ 🤷      0   \n",
       "...                                                  ...   ...    ...   \n",
       "14184  không mua hối hận đừng kêu nhá hàng ...     😂      1   \n",
       "14186  khuyên xinh lắm lúc đặt còn được dea...     😘      1   \n",
       "14193  đẹp rẻ chất lượng vô cùng xịn mãi u...     😗      1   \n",
       "14194                     đẹp sẽ ủng hộ tiếp nhé     👕      1   \n",
       "14200  mua về cho chồng mà con em mặc thử tr...     😂      1   \n",
       "\n",
       "                                            decode_emoji  \n",
       "4                                                    cry  \n",
       "15                cursing_face cursing_face cursing_face  \n",
       "30                                             roll_eyes  \n",
       "43     thumbsup thumbsup thumbsup thumbsup thumbsup t...  \n",
       "47                                     shrug female_sign  \n",
       "...                                                  ...  \n",
       "14184                                                joy  \n",
       "14186          kissing_heart kissing_heart kissing_heart  \n",
       "14193  kissing kissing kissing kissing kissing kissin...  \n",
       "14194  tshirt tshirt tshirt tshirt tshirt tshirt tshi...  \n",
       "14200                                                joy  \n",
       "\n",
       "[1283 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bước này, ta thực hiện chia `emojis` ra thành hai tập training data và test data với test data chiếm 20% số lượng các observer của `emojis`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['decode_emoji'], data['label'], test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1521                                            heart_eyes\n",
       "11485    hearts hearts hearts kissing_heart kissing_hea...\n",
       "3467                                              relieved\n",
       "7740     persevere persevere persevere persevere persev...\n",
       "12870                                          heart heart\n",
       "                               ...                        \n",
       "12351                                           gift_heart\n",
       "12799                                                heart\n",
       "13056                                            hugs wink\n",
       "10730    blush blush blush blush blush blush blush blus...\n",
       "13032                                                 wink\n",
       "Name: decode_emoji, Length: 1026, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bây giờ, chúng ta thực hiện kĩ thuật **vectorizing text**, ta áp dụng lần lượt hai phương pháp là **Bag of Words** và **TF-IDF**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Bag of words\n",
    "bow_vec = CountVectorizer()\n",
    "bow_emojis = bow_vec.fit_transform(X_train)\n",
    "\n",
    "print(bow_vec.get_feature_names())\n",
    "print(bow_emojis.toarray())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['100', 'anger', 'angry', 'ballet_shoes', 'bangbang', 'birthday', 'blossom', 'blue_heart', 'blush', 'bouquet', 'bow', 'broken_heart', 'butterfly', 'cake', 'camera', 'carrot', 'cherry_blossom', 'clap', 'cloud_with_rain', 'clown_face', 'cold_face', 'cold_sweat', 'compass', 'confounded', 'confused', 'cow', 'crossed_fingers', 'cry', 'crying_cat_face', 'cursing_face', 'disappointed', 'disappointed_relieved', 'dizzy', 'dizzy_face', 'dog', 'dog2', 'dollar', 'eagle', 'expressionless', 'face_with_thermometer', 'facepalm', 'fallen_leaf', 'fearful', 'female_sign', 'fire', 'fist_left', 'floppy_disk', 'flushed', 'four_leaf_clover', 'free', 'frowning_face', 'fu', 'ghost', 'gift_heart', 'golf', 'green_heart', 'green_salad', 'grimacing', 'grin', 'grinning', 'haircut', 'hand_over_mouth', 'hatched_chick', 'heart', 'heart_eyes', 'heart_eyes_cat', 'heartbeat', 'heartpulse', 'hearts', 'heavy_check_mark', 'heavy_heart_exclamation', 'hibiscus', 'high_brightness', 'hugs', 'information_desk_person', 'innocent', 'joy', 'joy_cat', 'kiss', 'kissing', 'kissing_cat', 'kissing_closed_eyes', 'kissing_heart', 'kissing_smiling_eyes', 'knife', 'lobster', 'lollipop', 'loud_sound', 'male_sign', 'man_facepalming', 'mask', 'moneybag', 'musical_score', 'nauseated_face', 'nerd_face', 'neutral_face', 'no_good', 'no_mouth', 'octopus', 'ok_hand', 'ok_man', 'ok_person', 'ok_woman', 'older_man', 'open_mouth', 'partying_face', 'pensive', 'persevere', 'pig_nose', 'pleading_face', 'point_left', 'point_right', 'potted_plant', 'pout', 'pray', 'purple_heart', 'pushpin', 'raised_eyebrow', 'raising_hand_woman', 'red_circle', 'relaxed', 'relieved', 'revolving_hearts', 'ribbon', 'rice', 'rice_ball', 'rofl', 'roll_eyes', 'satisfied', 'scream', 'see_no_evil', 'shamrock', 'shit', 'shrug', 'skull_and_crossbones', 'sleeping', 'sleepy', 'slightly_frowning_face', 'slightly_smiling_face', 'smile', 'smiley', 'smiley_cat', 'smiling_face_with_tear', 'smiling_face_with_three_hearts', 'smiling_imp', 'smirk', 'sneezing_face', 'sob', 'soon', 'sos', 'spaghetti', 'sparkles', 'sparkling_heart', 'speak_no_evil', 'squid', 'star', 'star2', 'star_struck', 'strawberry', 'stuck_out_tongue', 'stuck_out_tongue_closed_eyes', 'stuck_out_tongue_winking_eye', 'sun_behind_rain_cloud', 'sun_with_face', 'sunglasses', 'sweat', 'sweat_drops', 'sweat_smile', 'tada', 'telephone_receiver', 'thinking', 'thumbsdown', 'thumbsup', 'tired_face', 'triumph', 'tshirt', 'two_hearts', 'unamused', 'upside_down_face', 'vietnam', 'vomiting_face', 'walking', 'wave', 'weary', 'white_check_mark', 'white_heart', 'wink', 'woman_facepalming', 'woman_shrugging', 'woozy_face', 'worried', 'yellow_heart', 'yum', 'zany_face', 'zipper_mouth_face']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_emojis = tfidf_vec.fit_transform(X_train)\n",
    "\n",
    "print(tfidf_emojis.toarray())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "X_vectorizers = [\n",
    "    ('Bag of Words', bow_emojis),\n",
    "    ('TF-IDF', tfidf_emojis)\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bây giờ ta liệt kê tất cả các model mà ta có thể ap dụng vào bài toán Binary Classification vào một list object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "lst_models = [\n",
    "    ('Logistic Regression - [solver: lbfgs]', LogisticRegression(solver='lbfgs')),\n",
    "    ('Logistic Regression - [solver: liblinear]', LogisticRegression(solver='liblinear')),\n",
    "    ('Logistic Regression - [solver: newton-cg]', LogisticRegression(solver='newton-cg')),\n",
    "    ('KNN - [n_neighbors: 2]', KNeighborsClassifier(n_neighbors=2)),\n",
    "    ('KNN - [n_neighbors: 3]', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('SVC - [kernel: linear]', SVC(kernel='linear', random_state=42)),\n",
    "    ('SVC - [kernel: poly]', SVC(kernel='poly', random_state=42)),\n",
    "    ('SVC - [kernel: rbf]', SVC(kernel='rbf', random_state=42)),\n",
    "    ('SVC - [kernel: sigmoid]', SVC(kernel='sigmoid', random_state=42)),\n",
    "    ('Bernoulli', BernoulliNB()),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('AdaBoost', AdaBoostClassifier(base_estimator=RandomForestClassifier(random_state=42), random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='mlogloss'))\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def train(lst_models, X_vectorizer, y):\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    res_table = []\n",
    "    for vec_name, X in X_vectorizer:\n",
    "        for mdl_name, model in lst_models:\n",
    "            cv_res = cross_validate(model, X, y, cv=cv, return_train_score=True, scoring=['accuracy', 'roc_auc'])\n",
    "            res_table.append([vec_name, mdl_name,\n",
    "                              cv_res['train_accuracy'].mean(),\n",
    "                              cv_res['test_accuracy'].mean(),\n",
    "                              np.abs(cv_res['train_accuracy'].mean() - cv_res['test_accuracy'].mean()),\n",
    "                              cv_res['train_accuracy'].std(),\n",
    "                              cv_res['test_accuracy'].std(),\n",
    "                              cv_res['train_roc_auc'].mean(),\n",
    "                              cv_res['test_roc_auc'].mean(),\n",
    "                              np.abs(cv_res['train_roc_auc'].mean() - cv_res['test_roc_auc'].mean()),\n",
    "                              cv_res['train_roc_auc'].std(),\n",
    "                              cv_res['test_roc_auc'].std(),\n",
    "                              cv_res['fit_time'].mean()\n",
    "            ])\n",
    "    \n",
    "    res_table = pd.DataFrame(res_table, columns=['vectorizer', 'model', 'train_acc', 'test_acc', 'diff_acc',\n",
    "                                                 'train_acc_std', 'test_acc_std', 'train_roc_auc', 'test_roc_auc',\n",
    "                                                 'diff_roc_auc', 'train_roc_auc_std', 'test_roc_auc_std', 'fit_time'])\n",
    "    res_table.sort_values(by=['test_acc', 'test_roc_auc'], ascending=False, inplace=True)\n",
    "    return res_table.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "res_models = train(lst_models, X_vectorizers, y_train)\n",
    "\n",
    "res_models"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>diff_acc</th>\n",
       "      <th>train_acc_std</th>\n",
       "      <th>test_acc_std</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>diff_roc_auc</th>\n",
       "      <th>train_roc_auc_std</th>\n",
       "      <th>test_roc_auc_std</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVC - [kernel: rbf]</td>\n",
       "      <td>0.893546</td>\n",
       "      <td>0.851875</td>\n",
       "      <td>0.041671</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.038218</td>\n",
       "      <td>0.905123</td>\n",
       "      <td>0.873282</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.016902</td>\n",
       "      <td>0.033987</td>\n",
       "      <td>0.014874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Logistic Regression - [solver: liblinear]</td>\n",
       "      <td>0.890189</td>\n",
       "      <td>0.850866</td>\n",
       "      <td>0.039322</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.930616</td>\n",
       "      <td>0.903272</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.036520</td>\n",
       "      <td>0.001866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Logistic Regression - [solver: lbfgs]</td>\n",
       "      <td>0.890189</td>\n",
       "      <td>0.850866</td>\n",
       "      <td>0.039322</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.930633</td>\n",
       "      <td>0.903188</td>\n",
       "      <td>0.027445</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>0.012641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Logistic Regression - [solver: newton-cg]</td>\n",
       "      <td>0.890189</td>\n",
       "      <td>0.850866</td>\n",
       "      <td>0.039322</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.930635</td>\n",
       "      <td>0.903188</td>\n",
       "      <td>0.027447</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>0.017942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.900802</td>\n",
       "      <td>0.847944</td>\n",
       "      <td>0.052858</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.045535</td>\n",
       "      <td>0.957656</td>\n",
       "      <td>0.897262</td>\n",
       "      <td>0.060395</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.209228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVC - [kernel: poly]</td>\n",
       "      <td>0.896578</td>\n",
       "      <td>0.847002</td>\n",
       "      <td>0.049576</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.039243</td>\n",
       "      <td>0.905438</td>\n",
       "      <td>0.854690</td>\n",
       "      <td>0.050747</td>\n",
       "      <td>0.006277</td>\n",
       "      <td>0.044771</td>\n",
       "      <td>0.014134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.882933</td>\n",
       "      <td>0.845012</td>\n",
       "      <td>0.037920</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.034061</td>\n",
       "      <td>0.937264</td>\n",
       "      <td>0.898412</td>\n",
       "      <td>0.038853</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.032872</td>\n",
       "      <td>0.001586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.882933</td>\n",
       "      <td>0.845012</td>\n",
       "      <td>0.037920</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.034061</td>\n",
       "      <td>0.937264</td>\n",
       "      <td>0.898412</td>\n",
       "      <td>0.038853</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.032872</td>\n",
       "      <td>0.001950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVC - [kernel: sigmoid]</td>\n",
       "      <td>0.881200</td>\n",
       "      <td>0.844080</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>0.037964</td>\n",
       "      <td>0.908762</td>\n",
       "      <td>0.881717</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>0.012880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVC - [kernel: linear]</td>\n",
       "      <td>0.887698</td>\n",
       "      <td>0.843118</td>\n",
       "      <td>0.044580</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.036253</td>\n",
       "      <td>0.908829</td>\n",
       "      <td>0.870945</td>\n",
       "      <td>0.037883</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.034686</td>\n",
       "      <td>0.010844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.900802</td>\n",
       "      <td>0.842119</td>\n",
       "      <td>0.058683</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.045126</td>\n",
       "      <td>0.962721</td>\n",
       "      <td>0.871233</td>\n",
       "      <td>0.091488</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>12.711624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>KNN - [n_neighbors: 3]</td>\n",
       "      <td>0.873727</td>\n",
       "      <td>0.834304</td>\n",
       "      <td>0.039423</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>0.048597</td>\n",
       "      <td>0.906049</td>\n",
       "      <td>0.850616</td>\n",
       "      <td>0.055433</td>\n",
       "      <td>0.005915</td>\n",
       "      <td>0.048967</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.833343</td>\n",
       "      <td>0.094857</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.043493</td>\n",
       "      <td>0.975427</td>\n",
       "      <td>0.878510</td>\n",
       "      <td>0.096917</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.029488</td>\n",
       "      <td>0.202526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Logistic Regression - [solver: newton-cg]</td>\n",
       "      <td>0.880982</td>\n",
       "      <td>0.827489</td>\n",
       "      <td>0.053493</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.935250</td>\n",
       "      <td>0.884170</td>\n",
       "      <td>0.051080</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.036731</td>\n",
       "      <td>0.028815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Logistic Regression - [solver: lbfgs]</td>\n",
       "      <td>0.880982</td>\n",
       "      <td>0.827489</td>\n",
       "      <td>0.053493</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.935252</td>\n",
       "      <td>0.884128</td>\n",
       "      <td>0.051124</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.036914</td>\n",
       "      <td>0.023275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Logistic Regression - [solver: liblinear]</td>\n",
       "      <td>0.880765</td>\n",
       "      <td>0.826518</td>\n",
       "      <td>0.054247</td>\n",
       "      <td>0.006240</td>\n",
       "      <td>0.031986</td>\n",
       "      <td>0.935249</td>\n",
       "      <td>0.884170</td>\n",
       "      <td>0.051080</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.036731</td>\n",
       "      <td>0.002355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.826518</td>\n",
       "      <td>0.101682</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.981883</td>\n",
       "      <td>0.857981</td>\n",
       "      <td>0.123902</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.032441</td>\n",
       "      <td>12.932513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>SVC - [kernel: linear]</td>\n",
       "      <td>0.884988</td>\n",
       "      <td>0.820741</td>\n",
       "      <td>0.064248</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>0.027824</td>\n",
       "      <td>0.925946</td>\n",
       "      <td>0.876632</td>\n",
       "      <td>0.049314</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.017519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.854992</td>\n",
       "      <td>0.818685</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.046203</td>\n",
       "      <td>0.920824</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.031060</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.040109</td>\n",
       "      <td>0.468371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>KNN - [n_neighbors: 3]</td>\n",
       "      <td>0.898744</td>\n",
       "      <td>0.808928</td>\n",
       "      <td>0.089816</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.026856</td>\n",
       "      <td>0.935404</td>\n",
       "      <td>0.859903</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.027268</td>\n",
       "      <td>0.001050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.861815</td>\n",
       "      <td>0.803055</td>\n",
       "      <td>0.058760</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.046481</td>\n",
       "      <td>0.930295</td>\n",
       "      <td>0.879968</td>\n",
       "      <td>0.050327</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.403061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>KNN - [n_neighbors: 2]</td>\n",
       "      <td>0.814702</td>\n",
       "      <td>0.774910</td>\n",
       "      <td>0.039793</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.042470</td>\n",
       "      <td>0.899022</td>\n",
       "      <td>0.835368</td>\n",
       "      <td>0.063654</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.001247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>KNN - [n_neighbors: 2]</td>\n",
       "      <td>0.823909</td>\n",
       "      <td>0.746678</td>\n",
       "      <td>0.077231</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>0.033918</td>\n",
       "      <td>0.926534</td>\n",
       "      <td>0.816691</td>\n",
       "      <td>0.109842</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>0.033904</td>\n",
       "      <td>0.001154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>SVC - [kernel: rbf]</td>\n",
       "      <td>0.755578</td>\n",
       "      <td>0.714382</td>\n",
       "      <td>0.041195</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.030526</td>\n",
       "      <td>0.919260</td>\n",
       "      <td>0.874206</td>\n",
       "      <td>0.045053</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.035317</td>\n",
       "      <td>0.019723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>SVC - [kernel: sigmoid]</td>\n",
       "      <td>0.689516</td>\n",
       "      <td>0.672501</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.022026</td>\n",
       "      <td>0.878113</td>\n",
       "      <td>0.847238</td>\n",
       "      <td>0.030875</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.016986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>SVC - [kernel: poly]</td>\n",
       "      <td>0.672623</td>\n",
       "      <td>0.660832</td>\n",
       "      <td>0.011791</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.822122</td>\n",
       "      <td>0.070585</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.045143</td>\n",
       "      <td>0.023206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      vectorizer                                      model  train_acc  \\\n",
       "0         TF-IDF                        SVC - [kernel: rbf]   0.893546   \n",
       "1         TF-IDF  Logistic Regression - [solver: liblinear]   0.890189   \n",
       "2         TF-IDF      Logistic Regression - [solver: lbfgs]   0.890189   \n",
       "3         TF-IDF  Logistic Regression - [solver: newton-cg]   0.890189   \n",
       "4         TF-IDF                              Random Forest   0.900802   \n",
       "5         TF-IDF                       SVC - [kernel: poly]   0.896578   \n",
       "6   Bag of Words                                  Bernoulli   0.882933   \n",
       "7         TF-IDF                                  Bernoulli   0.882933   \n",
       "8         TF-IDF                    SVC - [kernel: sigmoid]   0.881200   \n",
       "9         TF-IDF                     SVC - [kernel: linear]   0.887698   \n",
       "10        TF-IDF                                   AdaBoost   0.900802   \n",
       "11        TF-IDF                     KNN - [n_neighbors: 3]   0.873727   \n",
       "12  Bag of Words                              Random Forest   0.928200   \n",
       "13  Bag of Words  Logistic Regression - [solver: newton-cg]   0.880982   \n",
       "14  Bag of Words      Logistic Regression - [solver: lbfgs]   0.880982   \n",
       "15  Bag of Words  Logistic Regression - [solver: liblinear]   0.880765   \n",
       "16  Bag of Words                                   AdaBoost   0.928200   \n",
       "17  Bag of Words                     SVC - [kernel: linear]   0.884988   \n",
       "18        TF-IDF                                    XGBoost   0.854992   \n",
       "19  Bag of Words                     KNN - [n_neighbors: 3]   0.898744   \n",
       "20  Bag of Words                                    XGBoost   0.861815   \n",
       "21        TF-IDF                     KNN - [n_neighbors: 2]   0.814702   \n",
       "22  Bag of Words                     KNN - [n_neighbors: 2]   0.823909   \n",
       "23  Bag of Words                        SVC - [kernel: rbf]   0.755578   \n",
       "24  Bag of Words                    SVC - [kernel: sigmoid]   0.689516   \n",
       "25  Bag of Words                       SVC - [kernel: poly]   0.672623   \n",
       "\n",
       "    test_acc  diff_acc  train_acc_std  test_acc_std  train_roc_auc  \\\n",
       "0   0.851875  0.041671       0.003487      0.038218       0.905123   \n",
       "1   0.850866  0.039322       0.004251      0.033072       0.930616   \n",
       "2   0.850866  0.039322       0.004251      0.033072       0.930633   \n",
       "3   0.850866  0.039322       0.004251      0.033072       0.930635   \n",
       "4   0.847944  0.052858       0.003933      0.045535       0.957656   \n",
       "5   0.847002  0.049576       0.003524      0.039243       0.905438   \n",
       "6   0.845012  0.037920       0.003072      0.034061       0.937264   \n",
       "7   0.845012  0.037920       0.003072      0.034061       0.937264   \n",
       "8   0.844080  0.037121       0.003237      0.037964       0.908762   \n",
       "9   0.843118  0.044580       0.003094      0.036253       0.908829   \n",
       "10  0.842119  0.058683       0.003933      0.045126       0.962721   \n",
       "11  0.834304  0.039423       0.004453      0.048597       0.906049   \n",
       "12  0.833343  0.094857       0.003246      0.043493       0.975427   \n",
       "13  0.827489  0.053493       0.006239      0.031486       0.935250   \n",
       "14  0.827489  0.053493       0.006239      0.031486       0.935252   \n",
       "15  0.826518  0.054247       0.006240      0.031986       0.935249   \n",
       "16  0.826518  0.101682       0.003246      0.037255       0.981883   \n",
       "17  0.820741  0.064248       0.007880      0.027824       0.925946   \n",
       "18  0.818685  0.036308       0.006688      0.046203       0.920824   \n",
       "19  0.808928  0.089816       0.003685      0.026856       0.935404   \n",
       "20  0.803055  0.058760       0.005243      0.046481       0.930295   \n",
       "21  0.774910  0.039793       0.016428      0.042470       0.899022   \n",
       "22  0.746678  0.077231       0.018303      0.033918       0.926534   \n",
       "23  0.714382  0.041195       0.005464      0.030526       0.919260   \n",
       "24  0.672501  0.017015       0.003921      0.022026       0.878113   \n",
       "25  0.660832  0.011791       0.001305      0.009961       0.892707   \n",
       "\n",
       "    test_roc_auc  diff_roc_auc  train_roc_auc_std  test_roc_auc_std   fit_time  \n",
       "0       0.873282      0.031841           0.016902          0.033987   0.014874  \n",
       "1       0.903272      0.027344           0.003066          0.036520   0.001866  \n",
       "2       0.903188      0.027445           0.003078          0.036723   0.012641  \n",
       "3       0.903188      0.027447           0.003078          0.036723   0.017942  \n",
       "4       0.897262      0.060395           0.002273          0.035206   0.209228  \n",
       "5       0.854690      0.050747           0.006277          0.044771   0.014134  \n",
       "6       0.898412      0.038853           0.002624          0.032872   0.001586  \n",
       "7       0.898412      0.038853           0.002624          0.032872   0.001950  \n",
       "8       0.881717      0.027045           0.010184          0.043808   0.012880  \n",
       "9       0.870945      0.037883           0.011004          0.034686   0.010844  \n",
       "10      0.871233      0.091488           0.002259          0.044248  12.711624  \n",
       "11      0.850616      0.055433           0.005915          0.048967   0.001244  \n",
       "12      0.878510      0.096917           0.001451          0.029488   0.202526  \n",
       "13      0.884170      0.051080           0.003135          0.036731   0.028815  \n",
       "14      0.884128      0.051124           0.003135          0.036914   0.023275  \n",
       "15      0.884170      0.051080           0.003132          0.036731   0.002355  \n",
       "16      0.857981      0.123902           0.001446          0.032441  12.932513  \n",
       "17      0.876632      0.049314           0.003860          0.037206   0.017519  \n",
       "18      0.889764      0.031060           0.003258          0.040109   0.468371  \n",
       "19      0.859903      0.075500           0.003627          0.027268   0.001050  \n",
       "20      0.879968      0.050327           0.003141          0.031373   0.403061  \n",
       "21      0.835368      0.063654           0.007371          0.052375   0.001247  \n",
       "22      0.816691      0.109842           0.005107          0.033904   0.001154  \n",
       "23      0.874206      0.045053           0.002626          0.035317   0.019723  \n",
       "24      0.847238      0.030875           0.010735          0.031568   0.016986  \n",
       "25      0.822122      0.070585           0.009763          0.045143   0.023206  "
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Nhận xét**:\n",
    "> * Bảng trên là thống kê về các chỉ số **accuracy** và **ROG-AUC** của 26 model được **sắp xếp giảm dần** dựa trên **valuation accuracy** _(trong sklearn định nghĩa là `test_accuracy`)_ và **valuation ROC-AUC** _(trong sklearn định nghĩa là `test_roc_auc`)_.\n",
    "> * Có thể thấy, sự khác biệt giữa các metric trên **train** và **valuation** là không quá lớn _(xem vào `diff_acc` và `diff_roc_auc`)_, trung bình có thể thấy chúng dao động dưới 5%. $\\Rightarrow$ Các model của chúng ta không bị overfitting.\n",
    "> * Số ít ở các model KNN và AdaBoost có các chỉ số đánh giá cho kết quả không tốt vì chúng dao động xung quanh 10% $\\Rightarrow$ Hai thuật toán này không phù hợp với bài toán Emoji sentiment analysis trên dataset của chúng ta.\n",
    "> * Phương pháp text vectorizing **TF-IDF** hoạt động tốt hơn **Bag of words**, có thể thấy ở 10 model cho hiệu suất tốt nhất thì phương pháp **TF-IDF** chiếm lĩnh toàn bộ trong việc đưa ra một cách tối ưu hóa đầu vào tốt trên các model.\n",
    "> * Về evaluation, cả **train/valuation accuracy** và **train/valuation ROC-AUC** khá tốt, luôn thuộc phạm vi $[80, 100]\\%$ ở 10 model đầu tiên. \n",
    "> * Như vậy, các model của chúng ta không bị overfitting cũng như bias và variance error. Ta có thể chọn ra 5 model đầu tiên để tiến hành Hyperparameters Optimization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}